{"cells":[{"cell_type":"code","execution_count":28,"metadata":{"id":"WJ0oSikkCagm"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"cC90Zc1JCag0"},"outputs":[],"source":["# For MAC with SSL error see: https://github.com/tensorflow/tensorflow/issues/33285\n","mnist = tf.keras.datasets.mnist\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","\n","# Each row in x_train is a 28x28 image with 784 pixels\n","# Normalize the data\n","# 225 is the max value of a pixel, we divide by 255 to get a value between 0 and 1 \n","# 0 is black, 1 is white\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","# number of training data\n","ndata_train = x_train.shape[0]\n","# number of test data\n","ndata_test = x_test.shape[0]"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"lUUkaOOAAHUO"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Plot 10th image in the training set\n","plt.pcolor( 1-x_train[9,::-1,:] , cmap = 'gray' );\n","plt.axis('off');"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"TZEeLbIbXLG9"},"outputs":[{"data":{"text/plain":["4"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# The actual label for the 10th image\n","y_train[9]"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"WLk18_VGCag4"},"outputs":[{"data":{"text/plain":["(28, 28, 1)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# Reshape the data to 4D tensor - (sample_number, x_img_size, y_img_size, num_channels)\n","# We have only one color channel for gray-scale images\n","x_train = x_train.reshape((ndata_train,28,28,1))\n","x_test = x_test.reshape((ndata_test,28,28,1))\n","\n","# Find the shape of input images\n","# Index 0 is the number of images, index 1 is the number of rows, index 2 is the number of columns, index 3 is the number of channels\n","# Here we only want the shape of the images and the number of channels, so we will use index 1 and 3\n","xshape = x_train.shape[1:4]\n","xshape\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"lmqZqqbMCag8"},"outputs":[],"source":["NNmodel = tf.keras.models.Sequential([\n","        # input layer, we flatten the 28x28 image into a 784 vector\n","        tf.keras.layers.Flatten(input_shape=xshape),\n","        # relu good for deep hidden layers\n","        tf.keras.layers.Dense(64,activation=tf.nn.relu),\n","        # sigmoid is also okay if not too many layers\n","        tf.keras.layers.Dense(64,activation=tf.nn.sigmoid),\n","        # softmax is good for output layer, left with 10 numbers that sum to 1\n","        # gives probability of each number\n","        tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n","        ])"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"WpONfujBCag_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_2 (Flatten)         (None, 784)               0         \n","                                                                 \n"," dense_6 (Dense)             (None, 64)                50240     \n","                                                                 \n"," dense_7 (Dense)             (None, 64)                4160      \n","                                                                 \n"," dense_8 (Dense)             (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 55,050\n","Trainable params: 55,050\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# sparse_categorical_crossentropy is good for classification with multiple classes\n","# sparse because we only have numbers from 0 to 9\n","NNmodel.compile(optimizer='adam',\n","                loss='sparse_categorical_crossentropy',\n","                metrics=['accuracy'])\n","\n","NNmodel.summary()"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"_zEmMpqvCahB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/12\n","240/240 [==============================] - 2s 7ms/step - loss: 0.8501 - accuracy: 0.7994 - val_loss: 0.3416 - val_accuracy: 0.9137\n","Epoch 2/12\n","240/240 [==============================] - 2s 9ms/step - loss: 0.3011 - accuracy: 0.9186 - val_loss: 0.2434 - val_accuracy: 0.9316\n","Epoch 3/12\n","240/240 [==============================] - 2s 8ms/step - loss: 0.2254 - accuracy: 0.9360 - val_loss: 0.1931 - val_accuracy: 0.9464\n","Epoch 4/12\n","240/240 [==============================] - 2s 7ms/step - loss: 0.1827 - accuracy: 0.9475 - val_loss: 0.1666 - val_accuracy: 0.9535\n","Epoch 5/12\n","240/240 [==============================] - 2s 6ms/step - loss: 0.1535 - accuracy: 0.9561 - val_loss: 0.1514 - val_accuracy: 0.9557\n","Epoch 6/12\n","240/240 [==============================] - 2s 6ms/step - loss: 0.1312 - accuracy: 0.9621 - val_loss: 0.1341 - val_accuracy: 0.9614\n","Epoch 7/12\n","240/240 [==============================] - 2s 6ms/step - loss: 0.1117 - accuracy: 0.9680 - val_loss: 0.1221 - val_accuracy: 0.9643\n","Epoch 8/12\n","240/240 [==============================] - 2s 6ms/step - loss: 0.0979 - accuracy: 0.9719 - val_loss: 0.1172 - val_accuracy: 0.9651\n","Epoch 9/12\n","240/240 [==============================] - 2s 6ms/step - loss: 0.0866 - accuracy: 0.9746 - val_loss: 0.1085 - val_accuracy: 0.9669\n","Epoch 10/12\n","240/240 [==============================] - 1s 6ms/step - loss: 0.0760 - accuracy: 0.9785 - val_loss: 0.1038 - val_accuracy: 0.9688\n","Epoch 11/12\n","240/240 [==============================] - 2s 6ms/step - loss: 0.0677 - accuracy: 0.9814 - val_loss: 0.1008 - val_accuracy: 0.9686\n","Epoch 12/12\n","240/240 [==============================] - 2s 6ms/step - loss: 0.0616 - accuracy: 0.9830 - val_loss: 0.0987 - val_accuracy: 0.9706\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x175be79d0>"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["# batch_size is the number of images to use for each gradient descent step\n","# epochs is the number of times to go through the entire training set\n","NNmodel.fit(x_train,y_train,epochs=12,validation_split=0.2,batch_size=200)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ucsh4BMTCahD"},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 2s 5ms/step - loss: 0.0950 - accuracy: 0.9711\n","This model predicts 97.11000323295593% of the test data correctly\n"]}],"source":["# Evaluate returns the classification accuracy\n","print('This model predicts '+str(NNmodel.evaluate(x_test,y_test)[1]*100) +'% of the test data correctly')"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"aRrLgOxXCahG"},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 1s 2ms/step\n"]},{"data":{"text/plain":["(10000, 10)"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["# Another way to evaluate the model\n","# Predict returns the probability of each class\n","pred_probs = NNmodel.predict(x_test)\n","pred_probs.shape"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"dcRSIEUcCahR"},"outputs":[{"data":{"text/plain":["1.0"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# Total of 10 probabilities for each image sum to 1\n","np.sum(pred_probs[0,:])"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"jSiGyR9QEpjR"},"outputs":[{"data":{"text/plain":["0.9711"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# Find the class with the highest probability\n","# Axis 0 is the image number, axis 1 is the class number\n","pred_class = np.argmax(pred_probs, axis=1)\n","# Compare with the true class, and take the mean to get the accuracy\n","np.mean(pred_class==y_test)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"rdfpHS5QCahV"},"outputs":[],"source":["# New model with convolutional layers and pooling layers\n","NNmodel2 = tf.keras.models.Sequential([\n","        # We pick the number of filters\n","        # We don't pick the pixel intensity, which is learned\n","        # Include filters layer\n","        tf.keras.layers.Conv2D(filters=10,kernel_size=(5,5),activation=tf.nn.relu,input_shape=xshape),\n","        # Pooling layer\n","        tf.keras.layers.MaxPooling2D(pool_size = (2,2),strides=2),\n","        # Flatten the data for the dense layers, where we stack the filters together\n","        tf.keras.layers.Flatten(),\n","        # Dense layers\n","        tf.keras.layers.Dense(128,activation=tf.nn.relu),\n","        tf.keras.layers.Dense(64,activation=tf.nn.softplus),\n","        tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n","        ])"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"caahsRsCCahX"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_1 (Conv2D)           (None, 24, 24, 10)        260       \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 12, 12, 10)       0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_3 (Flatten)         (None, 1440)              0         \n","                                                                 \n"," dense_9 (Dense)             (None, 128)               184448    \n","                                                                 \n"," dense_10 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dense_11 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 193,614\n","Trainable params: 193,614\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["NNmodel2.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","NNmodel2.summary()"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"eVhxwOPGCahZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","96/96 [==============================] - 2s 20ms/step - loss: 0.7740 - accuracy: 0.7748 - val_loss: 0.1990 - val_accuracy: 0.9450\n","Epoch 2/5\n","96/96 [==============================] - 2s 19ms/step - loss: 0.1648 - accuracy: 0.9508 - val_loss: 0.1211 - val_accuracy: 0.9662\n","Epoch 3/5\n","96/96 [==============================] - 2s 19ms/step - loss: 0.1107 - accuracy: 0.9670 - val_loss: 0.1021 - val_accuracy: 0.9712\n","Epoch 4/5\n","96/96 [==============================] - 2s 18ms/step - loss: 0.0846 - accuracy: 0.9752 - val_loss: 0.0837 - val_accuracy: 0.9754\n","Epoch 5/5\n","96/96 [==============================] - 2s 19ms/step - loss: 0.0697 - accuracy: 0.9794 - val_loss: 0.0843 - val_accuracy: 0.9762\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x299984160>"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# batch_size is the number of images to use for each gradient descent step\n","# epochs is the number of times to go through the entire training set\n","NNmodel2.fit(x_train,y_train,epochs=5,validation_split=0.2,batch_size=500)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 2s 6ms/step - loss: 0.0718 - accuracy: 0.9767\n","This model predicts 97.67000675201416% of the test data correctly\n"]}],"source":["# Evaluate returns the classification accuracy\n","print('This model predicts '+str(NNmodel2.evaluate(x_test,y_test)[1]*100) +'% of the test data correctly')"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"YkdlAs_4CahZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 1s 3ms/step\n","0.9767\n"]}],"source":["# Using our new model to find the probability of each class\n","pred_probs2 = NNmodel2.predict(x_test)\n","# Find the class with the highest probability\n","pred2 = np.argmax(pred_probs2, axis=1)\n","# Compare with the true class, and take the mean to get the accuracy\n","# Better than the first model by about 1%\n","print(np.mean(pred2==y_test))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
